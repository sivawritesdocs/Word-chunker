{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: total word count per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3842\n",
      "3627\n",
      "4590\n",
      "4590\n",
      "4763\n",
      "4532\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "def get_word_count(file):#this allows us to get the word count of each file\n",
    "    word_count=0\n",
    "    path=pathlib.Path(file)\n",
    "    fhandle=open(path)\n",
    "    for line in fhandle:\n",
    "        for line_word in line.split():\n",
    "            word_count+=1\n",
    "    return word_count\n",
    "        \n",
    "t1=get_word_count('data/Trump 2019.txt')\n",
    "print(t1)\n",
    "t2=get_word_count('data/Trump 2018.txt')\n",
    "print(t2)\n",
    "t3=get_word_count('data/Trump 2017.txt')\n",
    "print(t3)\n",
    "o1=get_word_count('data/Obama 2016.txt')\n",
    "print(o1)\n",
    "o2=get_word_count('data/Obama 2015.txt')\n",
    "print(o2)\n",
    "o3=get_word_count('data/Obama 2014.txt')\n",
    "print(o3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total average word length per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.076522644456013\n",
      "5.016818307140888\n",
      "4.9640522875816995\n",
      "4.9640522875816995\n",
      "4.951711106445518\n",
      "4.851279788172992\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "def word_characters(file):\n",
    "    word_char_len=0\n",
    "    path=pathlib.Path(file)\n",
    "    fhandle=open(path)\n",
    "    for line in fhandle:\n",
    "        for line_word in line.split():\n",
    "            word_char_len+=len(line_word) \n",
    "            \n",
    "    return word_char_len\n",
    "        \n",
    "Trum1=word_characters('data/Trump 2019.txt')\n",
    "aTrum1=Trum1/3842 #avg word length is number of characters divided by number of words\n",
    "print(aTrum1)\n",
    "Trum2=word_characters('data/Trump 2018.txt')\n",
    "aTrum2=Trum2/3627\n",
    "print(aTrum2)\n",
    "Trum3=word_characters('data/Trump 2017.txt')\n",
    "aTrum3=Trum3/4590\n",
    "print(aTrum3)\n",
    "\n",
    "Obam1=word_characters('data/Obama 2016.txt')\n",
    "aObam1=Obam1/4590\n",
    "print(aObam1)\n",
    "Obam2=word_characters('data/Obama 2015.txt')\n",
    "aObam2=Obam2/4763\n",
    "print(aObam2)\n",
    "Obam3=word_characters('data/Obama 2014.txt')\n",
    "aObam3=Obam3/4532\n",
    "print(aObam3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trigrams per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019---------\n",
      "[   (18, ('the', 'united', 'states')),\n",
      "    (4, ('the', 'world', 's')),\n",
      "    (4, ('iran', 's', 'leaders')),\n",
      "    (4, ('around', 'the', 'world')),\n",
      "    (3, ('will', 'not', 'be')),\n",
      "    (3, ('we', 'are', 'also')),\n",
      "    (3, ('united', 'states', 'is')),\n",
      "    (3, ('united', 'states', 'has')),\n",
      "    (3, ('the', 'western', 'hemisphere')),\n",
      "    (3, ('that', 'is', 'why'))]\n",
      "[   (18, ('the', 'united', 'states')),\n",
      "    (4, ('the', 'world', 's')),\n",
      "    (4, ('iran', 's', 'leaders')),\n",
      "    (4, ('around', 'the', 'world')),\n",
      "    (3, ('will', 'not', 'be')),\n",
      "    (3, ('we', 'are', 'also')),\n",
      "    (3, ('united', 'states', 'is')),\n",
      "    (3, ('united', 'states', 'has')),\n",
      "    (3, ('the', 'western', 'hemisphere')),\n",
      "    (3, ('that', 'is', 'why'))]\n",
      "None\n",
      "2018---------\n",
      "[   (23, ('the', 'united', 'states')),\n",
      "    (6, ('united', 'states', 'will')),\n",
      "    (6, ('united', 'states', 'is')),\n",
      "    (6, ('the', 'united', 'nations')),\n",
      "    (5, ('what', 'kind', 'of')),\n",
      "    (4, ('the', 'middle', 'east')),\n",
      "    (4, ('that', 'is', 'why')),\n",
      "    (4, ('states', 'will', 'not')),\n",
      "    (4, ('of', 'the', 'world')),\n",
      "    (3, ('will', 'find', 'new'))]\n",
      "[   (23, ('the', 'united', 'states')),\n",
      "    (6, ('united', 'states', 'will')),\n",
      "    (6, ('united', 'states', 'is')),\n",
      "    (6, ('the', 'united', 'nations')),\n",
      "    (5, ('what', 'kind', 'of')),\n",
      "    (4, ('the', 'middle', 'east')),\n",
      "    (4, ('that', 'is', 'why')),\n",
      "    (4, ('states', 'will', 'not')),\n",
      "    (4, ('of', 'the', 'world')),\n",
      "    (3, ('will', 'find', 'new'))]\n",
      "None\n",
      "2017---------\n",
      "[   (24, ('the', 'united', 'states')),\n",
      "    (16, ('the', 'united', 'nations')),\n",
      "    (7, ('of', 'the', 'world')),\n",
      "    (7, ('of', 'the', 'united')),\n",
      "    (6, ('united', 'states', 'has')),\n",
      "    (5, ('the', 'american', 'people')),\n",
      "    (5, ('it', 'is', 'time')),\n",
      "    (5, ('in', 'this', 'room')),\n",
      "    (5, ('in', 'the', 'united')),\n",
      "    (4, ('the', 'entire', 'world'))]\n",
      "[   (24, ('the', 'united', 'states')),\n",
      "    (16, ('the', 'united', 'nations')),\n",
      "    (7, ('of', 'the', 'world')),\n",
      "    (7, ('of', 'the', 'united')),\n",
      "    (6, ('united', 'states', 'has')),\n",
      "    (5, ('the', 'american', 'people')),\n",
      "    (5, ('it', 'is', 'time')),\n",
      "    (5, ('in', 'this', 'room')),\n",
      "    (5, ('in', 'the', 'united')),\n",
      "    (4, ('the', 'entire', 'world'))]\n",
      "None\n",
      "2016---------\n",
      "[   (24, ('the', 'united', 'states')),\n",
      "    (16, ('the', 'united', 'nations')),\n",
      "    (7, ('of', 'the', 'world')),\n",
      "    (7, ('of', 'the', 'united')),\n",
      "    (6, ('united', 'states', 'has')),\n",
      "    (5, ('the', 'american', 'people')),\n",
      "    (5, ('it', 'is', 'time')),\n",
      "    (5, ('in', 'this', 'room')),\n",
      "    (5, ('in', 'the', 'united')),\n",
      "    (4, ('the', 'entire', 'world'))]\n",
      "[   (24, ('the', 'united', 'states')),\n",
      "    (16, ('the', 'united', 'nations')),\n",
      "    (7, ('of', 'the', 'world')),\n",
      "    (7, ('of', 'the', 'united')),\n",
      "    (6, ('united', 'states', 'has')),\n",
      "    (5, ('the', 'american', 'people')),\n",
      "    (5, ('it', 'is', 'time')),\n",
      "    (5, ('in', 'this', 'room')),\n",
      "    (5, ('in', 'the', 'united')),\n",
      "    (4, ('the', 'entire', 'world'))]\n",
      "None\n",
      "2015---------\n",
      "[   (18, ('the', 'united', 'states')),\n",
      "    (8, ('i', 'believe', 'that')),\n",
      "    (5, ('united', 'states', 'is')),\n",
      "    (5, ('of', 'the', 'world')),\n",
      "    (4, ('will', 'continue', 'to')),\n",
      "    (4, ('think', 'of', 'the')),\n",
      "    (4, ('the', 'fact', 'that')),\n",
      "    (3, ('we', 'will', 'continue')),\n",
      "    (3, ('united', 'states', 'will')),\n",
      "    (3, ('the', 'world', 's'))]\n",
      "[   (18, ('the', 'united', 'states')),\n",
      "    (8, ('i', 'believe', 'that')),\n",
      "    (5, ('united', 'states', 'is')),\n",
      "    (5, ('of', 'the', 'world')),\n",
      "    (4, ('will', 'continue', 'to')),\n",
      "    (4, ('think', 'of', 'the')),\n",
      "    (4, ('the', 'fact', 'that')),\n",
      "    (3, ('we', 'will', 'continue')),\n",
      "    (3, ('united', 'states', 'will')),\n",
      "    (3, ('the', 'world', 's'))]\n",
      "None\n",
      "2014---------\n",
      "[   (7, ('of', 'the', 'world')),\n",
      "    (6, ('the', 'united', 'states')),\n",
      "    (5, ('the', 'people', 'of')),\n",
      "    (4, ('we', 'see', 'it')),\n",
      "    (4, ('the', 'middle', 'east')),\n",
      "    (4, ('see', 'it', 'in')),\n",
      "    (4, ('it', 'is', 'time')),\n",
      "    (3, ('world', 'in', 'which')),\n",
      "    (3, ('to', 'address', 'the')),\n",
      "    (3, ('people', 'of', 'the'))]\n",
      "[   (7, ('of', 'the', 'world')),\n",
      "    (6, ('the', 'united', 'states')),\n",
      "    (5, ('the', 'people', 'of')),\n",
      "    (4, ('we', 'see', 'it')),\n",
      "    (4, ('the', 'middle', 'east')),\n",
      "    (4, ('see', 'it', 'in')),\n",
      "    (4, ('it', 'is', 'time')),\n",
      "    (3, ('world', 'in', 'which')),\n",
      "    (3, ('to', 'address', 'the')),\n",
      "    (3, ('people', 'of', 'the'))]\n",
      "None\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pprint                                 # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)           # create a pretty printing object for debugging\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\") \n",
    "\n",
    "def give_me_trigrams(file):\n",
    "    fin = open(file, errors=\"ignore\") \n",
    "    s = fin.read()\n",
    "    list_of_trigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.trigrams(tokens))\n",
    "        list_of_trigrams.extend(res)\n",
    "    \n",
    "    d = dict()\n",
    "    for t in list_of_trigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    \n",
    "    lst = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst.append(t)\n",
    "\n",
    "    lst.sort(reverse=True)\n",
    "    #Print the top 10 most frequent trigrams\n",
    "    pp.pprint(lst[:10])\n",
    "    return              \n",
    "print(\"2019---------\")\n",
    "a=give_me_trigrams('data/Trump 2019.txt')\n",
    "print(give_me_trigrams('data/Trump 2019.txt'))\n",
    "print(\"2018---------\")\n",
    "b=give_me_trigrams('data/Trump 2018.txt')\n",
    "print(give_me_trigrams('data/Trump 2018.txt'))\n",
    "print(\"2017---------\")\n",
    "c=give_me_trigrams('data/Trump 2017.txt')\n",
    "print(give_me_trigrams('data/Trump 2017.txt'))\n",
    "print(\"2016---------\")\n",
    "d=give_me_trigrams('data/Obama 2016.txt')\n",
    "print(give_me_trigrams('data/Obama 2016.txt'))\n",
    "print(\"2015---------\")\n",
    "e=give_me_trigrams('data/Obama 2015.txt')\n",
    "print(give_me_trigrams('data/Obama 2015.txt'))\n",
    "print(\"2014---------\")\n",
    "f=give_me_trigrams('data/Obama 2014.txt')\n",
    "print(give_me_trigrams('data/Obama 2014.txt'))\n",
    "print(\"---------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigrams per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019---------\n",
      "[   (20, ('the', 'united')),\n",
      "    (19, ('the', 'world')),\n",
      "    (19, ('in', 'the')),\n",
      "    (18, ('united', 'states')),\n",
      "    (17, ('of', 'the')),\n",
      "    (15, ('we', 'are')),\n",
      "    (11, ('iran', 's')),\n",
      "    (10, ('we', 'will')),\n",
      "    (9, ('of', 'our')),\n",
      "    (8, ('will', 'be'))]\n",
      "[   (20, ('the', 'united')),\n",
      "    (19, ('the', 'world')),\n",
      "    (19, ('in', 'the')),\n",
      "    (18, ('united', 'states')),\n",
      "    (17, ('of', 'the')),\n",
      "    (15, ('we', 'are')),\n",
      "    (11, ('iran', 's')),\n",
      "    (10, ('we', 'will')),\n",
      "    (9, ('of', 'our')),\n",
      "    (8, ('will', 'be'))]\n",
      "None\n",
      "2018---------\n",
      "[   (29, ('the', 'united')),\n",
      "    (23, ('united', 'states')),\n",
      "    (15, ('the', 'world')),\n",
      "    (15, ('of', 'the')),\n",
      "    (14, ('and', 'the')),\n",
      "    (12, ('we', 'are')),\n",
      "    (12, ('in', 'the')),\n",
      "    (12, ('and', 'we')),\n",
      "    (11, ('we', 'will')),\n",
      "    (11, ('in', 'this'))]\n",
      "None\n",
      "2017---------\n",
      "[   (43, ('of', 'the')),\n",
      "    (40, ('the', 'united')),\n",
      "    (24, ('united', 'states')),\n",
      "    (20, ('to', 'the')),\n",
      "    (20, ('in', 'the')),\n",
      "    (20, ('for', 'the')),\n",
      "    (18, ('the', 'world')),\n",
      "    (17, ('united', 'nations')),\n",
      "    (15, ('it', 'is')),\n",
      "    (14, ('of', 'our'))]\n",
      "None\n",
      "2016---------\n",
      "[   (43, ('of', 'the')),\n",
      "    (40, ('the', 'united')),\n",
      "    (24, ('united', 'states')),\n",
      "    (20, ('to', 'the')),\n",
      "    (20, ('in', 'the')),\n",
      "    (20, ('for', 'the')),\n",
      "    (18, ('the', 'world')),\n",
      "    (17, ('united', 'nations')),\n",
      "    (15, ('it', 'is')),\n",
      "    (14, ('of', 'our'))]\n",
      "None\n",
      "2015---------\n",
      "[   (35, ('of', 'the')),\n",
      "    (21, ('the', 'united')),\n",
      "    (18, ('united', 'states')),\n",
      "    (17, ('the', 'world')),\n",
      "    (13, ('that', 'is')),\n",
      "    (13, ('it', 'is')),\n",
      "    (12, ('we', 'can')),\n",
      "    (12, ('that', 'the')),\n",
      "    (12, ('in', 'the')),\n",
      "    (12, ('for', 'the'))]\n",
      "None\n",
      "2014---------\n",
      "[   (27, ('of', 'the')),\n",
      "    (22, ('we', 'will')),\n",
      "    (22, ('the', 'world')),\n",
      "    (17, ('in', 'the')),\n",
      "    (11, ('those', 'who')),\n",
      "    (10, ('we', 'can')),\n",
      "    (10, ('to', 'be')),\n",
      "    (10, ('for', 'the')),\n",
      "    (10, ('at', 'the')),\n",
      "    (10, ('and', 'we'))]\n",
      "None\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pprint                                 # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)           # create a pretty printing object for debugging\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\") \n",
    "\n",
    "def give_me_bigrams(folder):\n",
    "    fin = open(folder, errors=\"ignore\") \n",
    "    s = fin.read()\n",
    "    list_of_bigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.bigrams(tokens))\n",
    "        list_of_bigrams.extend(res)\n",
    "    \n",
    "    d = dict()\n",
    "    for t in list_of_bigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    \n",
    "    lst = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst.append(t)\n",
    "\n",
    "    lst.sort(reverse=True)\n",
    "    #Print the top 10 most frequent trigrams\n",
    "    lst2=pp.pprint(lst[:10])\n",
    "    return lst2\n",
    "print(\"2019---------\")\n",
    "g=give_me_bigrams('data/Trump 2019.txt')\n",
    "print(give_me_bigrams('data/Trump 2019.txt'))\n",
    "print(\"2018---------\")\n",
    "h=give_me_bigrams('data/Trump 2018.txt')\n",
    "print(give_me_bigrams('data/Trump 2018.txt'))\n",
    "print(\"2017---------\")      \n",
    "i=give_me_bigrams('data/Trump 2017.txt')\n",
    "print(give_me_bigrams('data/Trump 2017.txt'))\n",
    "print(\"2016---------\")\n",
    "j=give_me_bigrams('data/Obama 2016.txt')\n",
    "print(give_me_bigrams('data/Obama 2016.txt'))\n",
    "print(\"2015---------\")\n",
    "k=give_me_bigrams('data/Obama 2015.txt')\n",
    "print(give_me_bigrams('data/Obama 2015.txt'))\n",
    "print(\"2014---------\")\n",
    "l=give_me_bigrams('data/Obama 2014.txt')\n",
    "print(give_me_bigrams('data/Obama 2014.txt'))\n",
    "print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import string                     \n",
    "import os\n",
    "import pathlib\n",
    "import pprint                                  # pretty prnting for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object used for debugging.\n",
    "\n",
    "puncts = string.punctuation\n",
    "\n",
    "# Briefly explain what this function does here:\n",
    "def get_function_words():\n",
    "    fhand = open(\"data/function_words.txt\") #opens a file\n",
    "    result = list()                            \n",
    "    for line in fhand:\n",
    "        result.append(line.strip()) #appends the lines in the file without punctuation to a list\n",
    "    return result\n",
    "\n",
    "# Briefly explain what this function does here: This is a fuction that detemtermines if a word is a function word or not\n",
    "fw = get_function_words()  \n",
    "def is_function_word(w):\n",
    "    if w in fw:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "#This strips punctuation by skipping all punctuation characters and adding characters to an empty string\n",
    "def strip_punct(s):\n",
    "    res = \"\"                       \n",
    "    for c in s:                    \n",
    "        if c in puncts:            \n",
    "            continue               \n",
    "        else:                      \n",
    "            res = res + c\n",
    "    return res                   \n",
    "\n",
    "def top_n_words(filename, n):#This line take a file name and word as arguments of the function\n",
    "    d = dict()#defines an empty dictionary\n",
    "    fin = open(filename)#open file\n",
    "    for w in fin.read().split():#convertfile lines to list\n",
    "    \n",
    "        if w == '':\n",
    "            continue\n",
    "        \n",
    "        clean_w = strip_punct(w)#calling stript_punct function\n",
    "        clean_w = clean_w.lower()#lowers all the cases of the function\n",
    "        if clean_w == '' or is_function_word(clean_w):# if clean_w is either an empty string or is a function word, skip it\n",
    "            continue\n",
    "        \n",
    "        if clean_w not in d:# count and add word to a dictionary\n",
    "            d[clean_w] = 1\n",
    "        else:\n",
    "            d[clean_w] = d[clean_w] + 1\n",
    "\n",
    "    # sort\n",
    "    lst = list()\n",
    "    for key, val in d.items():\n",
    "         lst.append((val, key))# append the dictionary contents to the list\n",
    "        \n",
    "    lst.sort(reverse=True)#print the list in reverse\n",
    "    return lst[:n]   \n",
    "\n",
    "\n",
    "def run(file):\n",
    "    big_dict = dict()\n",
    "    top = 0            \n",
    "    top10 = top_n_words(file, 10)\n",
    "    dotpos = file.find(\".\")\n",
    "    year = file[dotpos-4:dotpos]\n",
    "#     print(year)\n",
    "    for key, val in top10:\n",
    "#         print(key, val)\n",
    "        if key > top:\n",
    "            winner = (key, val, year)\n",
    "            top = key\n",
    "    return top10\n",
    "#     print(\"\")\n",
    "  \n",
    "    \n",
    "T1=run(\"data/Trump 2019.txt\")\n",
    "print('----')\n",
    "T2=run(\"data/Trump 2018.txt\")\n",
    "print('----')\n",
    "T3=run(\"data/Trump 2017.txt\")\n",
    "print('----')\n",
    "O1=run(\"data/Obama 2016.txt\")\n",
    "print('----')\n",
    "O2=run(\"data/Obama 2015.txt\")\n",
    "print('----')\n",
    "O3=run(\"data/Obama 2014.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # make sure to do this.\n",
    "import pathlib\n",
    "path = pathlib.Path(\"data\")    # create a path object for the 'text' folder\n",
    "files = path.glob(\"*.txt\")     # get a list of files with the \".txt\" extension in the folder.\n",
    "for file in files:             # for each file in the list of files,\n",
    "    print(file)                # print the filename.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26208\n",
      "4368.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib \n",
    "def word_counter(folder):\n",
    "    path=pathlib.Path(folder)\n",
    "    files=path.glob(\"*.txt\")\n",
    "    word_count=0\n",
    "    for file in files:\n",
    "        fhandle = open(file, errors=\"ignore\")\n",
    "        for line in fhandle:\n",
    "            for line_word in line.split():\n",
    "                word_count+=1\n",
    "    return word_count\n",
    "\n",
    "total_words=word_counter(\"data\")\n",
    "avg_words=total_words/6\n",
    "print(word_counter(\"data\"))\n",
    "print(avg_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of characters and avg word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130188\n",
      "4.967490842490842\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib \n",
    "def word_counter(folder):\n",
    "    path=pathlib.Path(folder)\n",
    "    files=path.glob(\"*.txt\")\n",
    "    word_count=0\n",
    "    for file in files:\n",
    "        fhandle = open(file, errors=\"ignore\")\n",
    "        for line in fhandle:\n",
    "            for line_word in line.split():\n",
    "                word_count+=1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "def word_length(folder):\n",
    "    path=pathlib.Path(folder)\n",
    "    files=path.glob(\"*.txt\")\n",
    "    word_count=0\n",
    "    for file in files:\n",
    "        fhandle = open(file, errors=\"ignore\")\n",
    "        for line in fhandle:\n",
    "            for line_word in line.split():\n",
    "                word_count+=len(line_word)\n",
    "    return word_count\n",
    "\n",
    "character_len=word_length(\"data\")\n",
    "total_words=word_counter(\"data\")\n",
    "avg_word_len=character_len/total_words\n",
    "print(character_len)\n",
    "print(avg_word_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten most popular words for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('people', 207), ('nations', 197), (('of', 'the'), 180), ('united', 164), (('the', 'united'), 156), ('world', 151), ('states', 123), (('united', 'states'), 113), (('the', 'world'), 109), (('in', 'the'), 100)]\n"
     ]
    }
   ],
   "source": [
    "import string                     \n",
    "import os\n",
    "import pprint                                  # pretty prnting for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object used for debugging.\n",
    "\n",
    "puncts = string.punctuation\n",
    "\n",
    "# Briefly explain what this function does here:\n",
    "def get_function_words():\n",
    "    fhand = open(\"data/function_words.txt\") #opens a file\n",
    "    result = list()                            \n",
    "    for line in fhand:\n",
    "        result.append(line.strip()) #appends the lines in the file without punctuation to a list\n",
    "    return result\n",
    "\n",
    "# Briefly explain what this function does here: This is a fuction that detemtermines if a word is a function word or not\n",
    "fw = get_function_words()  \n",
    "def is_function_word(w):\n",
    "    if w in fw:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Briefly explain what this function does here: This strips punctuation by skipping all punctuation characters and adding characters to an empty string\n",
    "def strip_punct(s):\n",
    "    res = \"\"                       \n",
    "    for c in s:                    \n",
    "        if c in puncts:            \n",
    "            continue               \n",
    "        else:                      \n",
    "            res = res + c\n",
    "    return res                   \n",
    "\n",
    "# Briefly explain what this function does here:\n",
    "# Also, add line-by-line comments below.\n",
    "def top_n_words(filename, n):#This line take a file name and word as arguments of the function\n",
    "    d =dict()#defines an empty dictionary\n",
    "    fin = open(filename)#open file\n",
    "    for w in fin.read().split():#convertfile lines to list\n",
    "    \n",
    "        if w == '':\n",
    "            continue\n",
    "        \n",
    "        clean_w = strip_punct(w)#calling stript_punct function\n",
    "        clean_w = clean_w.lower()#lowers all the cases of the function\n",
    "        if clean_w == '' or is_function_word(clean_w):# if clean_w is either an empty string or is a function word, skip it\n",
    "            continue\n",
    "        \n",
    "        if clean_w not in d:# count and add word to a dictionary\n",
    "            d[clean_w] = 1\n",
    "        else:\n",
    "            d[clean_w] = d[clean_w] + 1\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "# Also, add line-by-line comments below.\n",
    "def run(file):\n",
    "    big_dict = dict()\n",
    "    top = 0            \n",
    "    top10 = top_n_words(file, 10)\n",
    "    dotpos = file.find(\".\")\n",
    "    year = file[dotpos-4:dotpos]\n",
    "    print(year)\n",
    "    for key, val in top10:\n",
    "        print(key, val)\n",
    "        if key > top:\n",
    "            winner = (key, val, year)\n",
    "            top = key\n",
    "    print(\"\")\n",
    "global_counter0= Counter()\n",
    "global_counter.update(top_n_words(\"data/Trump 2019.txt\", 10))\n",
    "global_counter.update(top_n_words(\"data/Trump 2018.txt\", 10))\n",
    "global_counter.update(top_n_words(\"data/Trump 2017.txt\", 10))\n",
    "global_counter.update(top_n_words(\"data/Obama 2016.txt\", 10))\n",
    "global_counter.update(top_n_words(\"data/Obama 2015.txt\", 10))\n",
    "global_counter.update(top_n_words(\"data/Obama 2014.txt\", 10))\n",
    "print (global_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ten trigrams for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'united', 'states'), 113), (('the', 'united', 'nations'), 42), (('of', 'the', 'world'), 32), (('united', 'states', 'is'), 21), (('united', 'states', 'has'), 19), (('of', 'the', 'united'), 17), (('it', 'is', 'time'), 16), (('in', 'the', 'united'), 15), (('around', 'the', 'world'), 14), (('united', 'states', 'will'), 14)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pprint                                 # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)           # create a pretty printing object for debugging\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\") \n",
    "\n",
    "def give_me_total_trigrams(file):\n",
    "    fin = open(file, errors=\"ignore\") \n",
    "    s = fin.read()\n",
    "    list_of_trigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.trigrams(tokens))\n",
    "        list_of_trigrams.extend(res)\n",
    "    \n",
    "    d = Counter()\n",
    "    for t in list_of_trigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    \n",
    "    return d\n",
    "    \n",
    "\n",
    "global_counter1 = Counter()\n",
    "global_counter1.update(give_me_total_trigrams('data/Trump 2019.txt'))\n",
    "global_counter1.update(give_me_total_trigrams('data/Trump 2018.txt'))\n",
    "global_counter1.update(give_me_total_trigrams('data/Trump 2017.txt'))\n",
    "global_counter1.update(give_me_total_trigrams('data/Obama 2016.txt'))\n",
    "global_counter1.update(give_me_total_trigrams('data/Obama 2015.txt'))\n",
    "global_counter1.update(give_me_total_trigrams('data/Obama 2014.txt'))\n",
    "\n",
    "print (global_counter1.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten bigrams for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 180), (('the', 'united'), 156), (('united', 'states'), 113), (('the', 'world'), 109), (('in', 'the'), 100), (('for', 'the'), 76), (('to', 'the'), 70), (('we', 'will'), 69), (('and', 'the'), 63), (('it', 'is'), 61)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pprint                                 # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)           # create a pretty printing object for debugging\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\") \n",
    "\n",
    "def give_me_total_bigrams(file):\n",
    "    fin = open(file, errors=\"ignore\") \n",
    "    s = fin.read()\n",
    "    list_of_bigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.bigrams(tokens))\n",
    "        list_of_bigrams.extend(res)\n",
    "    \n",
    "    d = Counter()\n",
    "    for t in list_of_bigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    \n",
    "    return d\n",
    "    \n",
    "\n",
    "global_counter = Counter()\n",
    "global_counter.update(give_me_total_bigrams('data/Trump 2019.txt'))\n",
    "global_counter.update(give_me_total_bigrams('data/Trump 2018.txt'))\n",
    "global_counter.update(give_me_total_bigrams('data/Trump 2017.txt'))\n",
    "global_counter.update(give_me_total_bigrams('data/Obama 2016.txt'))\n",
    "global_counter.update(give_me_total_bigrams('data/Obama 2015.txt'))\n",
    "global_counter.update(give_me_total_bigrams('data/Obama 2014.txt'))\n",
    "print (global_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def get_word_count(file):#this allows us to get the word count of each file\n",
    "    word_count=0\n",
    "    path=pathlib.Path(file)\n",
    "    fhandle=open(path)\n",
    "    for line in fhandle:\n",
    "        for line_word in line.split():\n",
    "            word_count+=1\n",
    "    return word_count\n",
    "# t1=get_word_count('data/Trump 2019.txt')\n",
    "# t2=get_word_count('data/Trump 2018.txt')\n",
    "# t3=get_word_count('data/Trump 2017.txt')\n",
    "# o1=get_word_count('data/Obama 2016.txt')\n",
    "# o2=get_word_count('data/Obama 2015.txt')\n",
    "# o3=get_word_count('data/Obama 2014.txt')\n",
    "import pathlib\n",
    "def word_characters(file):\n",
    "    word_char_len=0\n",
    "    path=pathlib.Path(file)\n",
    "    fhandle=open(path)\n",
    "    for line in fhandle:\n",
    "        for line_word in line.split():\n",
    "            word_char_len+=len(line_word) \n",
    "            \n",
    "    return word_char_len\n",
    "        \n",
    "# Trum1=word_characters('data/Trump 2019.txt')\n",
    "# aTrum1=Trum1/t1 #avg word length is number of characters divided by number of words\n",
    "# Trum2=word_characters('data/Trump 2018.txt')\n",
    "# aTrum2=Trum2/t2\n",
    "# Trum3=word_characters('data/Trump 2017.txt')\n",
    "# aTrum3=Trum3/t3\n",
    "\n",
    "# Obam1=word_characters('data/Obama 2016.txt')\n",
    "# aObam1=Obam1/o1\n",
    "# Obam2=word_characters('data/Obama 2015.txt')\n",
    "# aObam2=Obam2/o2\n",
    "# Obam3=word_characters('data/Obama 2014.txt')\n",
    "# aObam3=Obam3/o3\n",
    "\n",
    "import string\n",
    "import pprint                                 # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)           # create a pretty printing object for debugging\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\") \n",
    "\n",
    "def give_me_trigrams(file):\n",
    "    fin = open(file, errors=\"ignore\") \n",
    "    s = fin.read()\n",
    "    list_of_trigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.trigrams(tokens))\n",
    "        list_of_trigrams.extend(res)\n",
    "    \n",
    "    d = dict()\n",
    "    for t in list_of_trigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    \n",
    "    lst1 = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst1.append(t)\n",
    "\n",
    "    lst1.sort(reverse=True)\n",
    "    #Print the top 10 most frequent trigrams\n",
    "#     pp.pprint(lst1[:10])\n",
    "    return lst1[:10]\n",
    "\n",
    "\n",
    "def give_me_bigrams(folder):\n",
    "    fin = open(folder, errors=\"ignore\") \n",
    "    s = fin.read()\n",
    "    list_of_bigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.bigrams(tokens))\n",
    "        list_of_bigrams.extend(res)\n",
    "    \n",
    "    d = dict()\n",
    "    for t in list_of_bigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    \n",
    "    lst2 = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst2.append(t)\n",
    "\n",
    "    lst2.sort(reverse=True)\n",
    "    #Print the top 10 most frequent trigrams\n",
    "#     lst2=pp.pprint(lst2[:10])\n",
    "    return lst2[:10]\n",
    "\n",
    "def getEverything(file):\n",
    "    totalWords = get_word_count(file)\n",
    "    wordChars = word_characters(file)\n",
    "    trigrams = give_me_trigrams(file)\n",
    "    bigrams = give_me_bigrams(file)\n",
    "    top10Words = run(file)\n",
    "    \n",
    "    top10Words = ' '.join([x[1] for x in top10Words])\n",
    "    \n",
    "    trigrams = [x[1] for x in trigrams]\n",
    "    trigrams = ' '.join(['(' + ' '.join(x) + ')' for x in trigrams])\n",
    "    \n",
    "    bigrams = [x[1] for x in bigrams]\n",
    "    bigrams = ' '.join(['(' + ' '.join(x) + ')' for x in bigrams])\n",
    "\n",
    "    finalString = \"{},{},{},{},{}\\n\".format(totalWords, wordChars/(totalWords*1.0), top10Words, bigrams, trigrams)\n",
    "    return finalString    \n",
    "    \n",
    "\n",
    "fhandle=open('data/Corpus analysis1.csv', 'w')\n",
    "fhandle.write('File,Word count,Avg.word length,Top 10 words,Top 10 bigrams,Top 10 trigrams\\n')\n",
    "fhandle.write('Trump 2019,' + getEverything('data/Trump 2019.txt'))\n",
    "fhandle.write('Trump 2018,' + getEverything('data/Trump 2018.txt'))\n",
    "fhandle.write('Trump 2017,' + getEverything('data/Trump 2017.txt'))\n",
    "fhandle.write('Obama 2016,' + getEverything('data/Obama 2016.txt'))\n",
    "fhandle.write('Obama 2015,' + getEverything('data/Obama 2015.txt'))\n",
    "fhandle.write('Obama 2014,' + getEverything('data/Obama 2014.txt'))\n",
    "fhandle.write('Obama 2014,' + getEverything('data/Obama 2014'))\n",
    "\n",
    "fhandle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
